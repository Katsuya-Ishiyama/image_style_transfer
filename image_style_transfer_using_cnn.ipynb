{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Style Transferの実装\n",
    "\n",
    "--------------------------------\n",
    "Gatys, Ecker, and Bethge(2016)の画像スタイル変換を実装する。\n",
    "\n",
    "【参考文献】  \n",
    "L. A. Gatys, A. S. Ecker, and M. Bethge,  \n",
    "Image style transfer using convolutional neural networks,  \n",
    "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages, 2414-2423, 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CONTENT_FILE = '/home/ishiyama/image_style_transfer/image/input/test_input_01.JPG'\n",
    "STYLE_FILE = '/home/ishiyama/image_style_transfer/image/style/test_style_01.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_images_as_jpeg(file):\n",
    "    \n",
    "    \"\"\"\n",
    "    JPEG Image Reader\n",
    "    \n",
    "    This function reads the content and style images as JPEG format.\n",
    "    These image data must be square for now, different height and\n",
    "    width will be able to supplied for future.\n",
    "    \n",
    "    Args:\n",
    "        file : str. A path of the image file.\n",
    "    \n",
    "    Returns:\n",
    "        A tuple. Each Elements are Tensor object of the read images.\n",
    "    \"\"\"\n",
    "    \n",
    "    filename_queue = tf.train.string_input_producer([file])\n",
    "    reader = tf.WholeFileReader()\n",
    "    key, value = reader.read(filename_queue)\n",
    "    images = tf.image.decode_jpeg(value)\n",
    "\n",
    "    # Convert read image data to RGB representation.\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    tf.train.start_queue_runners(sess)\n",
    "    image_rgb = sess.run(image)\n",
    "\n",
    "    # Reshaping numpy.array to same format of 中井(2016) [CNN-03] line3 and 4(pp.205).\n",
    "    height, width, channels = image_rgb.shape\n",
    "    tmp = tf.placeholder(tf.float32, [None, height * width])\n",
    "    x = tf.reshape(tmp, [-1, height, width, channels])\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image = read_images_as_jpeg(\n",
    "    content_file=CONTENT_FILE,\n",
    "    style_file=STYLE_FILE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape_3:0' shape=(?, 1200, 1600, 3) dtype=float32>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGGを実装する\n",
    "------------------------\n",
    "画像の特徴量を抽出するアルゴリズムにはSimonyan and Zisserman(2015)で提案されたCNN(VGG19)の畳込み層とプーリング層が使われている。  \n",
    "ここでは、「TensorFlowで学ぶディープラーニング入門」の多層CNNの実装を参考にVGG19を構築する。  \n",
    "\n",
    "【参考文献】  \n",
    "K. Simonyan and A. Zisserman, Very Deep Convolutional Networks For Large-Scale Image Recognition, arXiv: 1409.1556v6, 2015  \n",
    "中井悦司, TensorFlowで学ぶディープラーニング入門〜畳み込みニューラルネットワーク徹底解説, マイナビ出版, 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 畳み込み層を実装する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "\n",
    "def calculate_convolutional_layer(x, filter_conf, output_channels):\n",
    "\n",
    "    \"\"\"\n",
    "    Executeing a convolutional layer task.\n",
    "    \n",
    "    Args:\n",
    "        x               : An image data.\n",
    "        filter_conf     : A dict, or a dict like. the configulations of convolution filter.\n",
    "                          This must have 4 keys (\"height\", \"width\", \"channels\" and \"num\").\n",
    "        output_channels : A number of channels which is output of this function.\n",
    "\n",
    "    Returns:\n",
    "        An activation of an convolutional layer.\n",
    "    \"\"\"\n",
    "\n",
    "    if len(filter_conf) != 4:\n",
    "        raise ValueError('filter_conf does not have enough elements.')\n",
    "\n",
    "    if not isinstance(filter_conf, dict):\n",
    "        raise TypeError('filter_conf must be the dict type, or a dict like.')\n",
    "\n",
    "    if (('height' not in filter_conf)\n",
    "        or ('width' not in filter_conf)\n",
    "        or ('channels' not in filter_conf)\n",
    "        or ('num' not in filter_conf)):\n",
    "        raise ValueError('filter_conf must have \"height\", \"width\", \"channels\" and \"num\".\\n'\n",
    "                         'Please specify these 4 conditions.')\n",
    "    \n",
    "    W = tf.Variable(\n",
    "        tf.truncated_normal(\n",
    "            [filter_conf['height'],\n",
    "             filter_conf['width'],\n",
    "             filter_conf['channels'],\n",
    "             filter_conf['num']],\n",
    "            stddev=0.1\n",
    "        )\n",
    "    )\n",
    "    h = tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    b = tf.Variable(tf.constant(0.1, shape=output_channels))\n",
    "    convoluted_image = tf.nn.relu(h + b)\n",
    "    \n",
    "    return convoluted_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Dimensions must be equal, but are 3 and 1 for 'Conv2D_9' (op: 'Conv2D') with input shapes: [?,1200,1600,3], [3,3,1,1].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/home/ishiyama/.pyenv/versions/anaconda3-4.0.0/lib/python3.5/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    670\u001b[0m           \u001b[0mgraph_def_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_def_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m           input_tensors_as_shapes, status)\n\u001b[0m\u001b[1;32m    672\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ishiyama/.pyenv/versions/anaconda3-4.0.0/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ishiyama/.pyenv/versions/anaconda3-4.0.0/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Dimensions must be equal, but are 3 and 1 for 'Conv2D_9' (op: 'Conv2D') with input shapes: [?,1200,1600,3], [3,3,1,1].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-8874cb28d6db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;34m'num'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     },\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0moutput_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m )\n",
      "\u001b[0;32m<ipython-input-45-e0104c18e059>\u001b[0m in \u001b[0;36mcalculate_convolutional_layer\u001b[0;34m(x, filter_conf, output_channels)\u001b[0m\n\u001b[1;32m     41\u001b[0m         )\n\u001b[1;32m     42\u001b[0m     )\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'SAME'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_channels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mconvoluted_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ishiyama/.pyenv/versions/anaconda3-4.0.0/lib/python3.5/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(input, filter, strides, padding, use_cudnn_on_gpu, data_format, name)\u001b[0m\n\u001b[1;32m    394\u001b[0m                                 \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                                 \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m                                 data_format=data_format, name=name)\n\u001b[0m\u001b[1;32m    397\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ishiyama/.pyenv/versions/anaconda3-4.0.0/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36mapply_op\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    761\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    762\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    764\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ishiyama/.pyenv/versions/anaconda3-4.0.0/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   2327\u001b[0m                     original_op=self._default_original_op, op_def=op_def)\n\u001b[1;32m   2328\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2329\u001b[0;31m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2330\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2331\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ishiyama/.pyenv/versions/anaconda3-4.0.0/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1715\u001b[0m       \u001b[0mshape_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1717\u001b[0;31m   \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1718\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1719\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;32m/home/ishiyama/.pyenv/versions/anaconda3-4.0.0/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcall_with_requiring\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1666\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1667\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall_cpp_shape_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_shape_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1669\u001b[0m   \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ishiyama/.pyenv/versions/anaconda3-4.0.0/lib/python3.5/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    608\u001b[0m     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,\n\u001b[1;32m    609\u001b[0m                                   \u001b[0minput_tensors_as_shapes_needed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m                                   debug_python_shape_fn, require_shape_fn)\n\u001b[0m\u001b[1;32m    611\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m       \u001b[0;31m# Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ishiyama/.pyenv/versions/anaconda3-4.0.0/lib/python3.5/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    674\u001b[0m       \u001b[0mmissing_shape_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmissing_shape_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Dimensions must be equal, but are 3 and 1 for 'Conv2D_9' (op: 'Conv2D') with input shapes: [?,1200,1600,3], [3,3,1,1]."
     ]
    }
   ],
   "source": [
    "calculate_convolutional_layer(\n",
    "    x=image,\n",
    "    filter_conf={\n",
    "        'height': 3,\n",
    "        'width': 3,\n",
    "        'channels': 1,\n",
    "        'num': 1\n",
    "    },\n",
    "    output_channels=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Maxプーリング層を実装する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_max_pooling_layer(x, ksize, strides):\n",
    "\n",
    "    \"\"\"Wrapper function of tf.nn.max_pool.\n",
    "    \n",
    "    Args:\n",
    "        x       : A Tensor produced by calculate_convolutional_layer.\n",
    "        ksize   : A list of ints that has length >= 4. The size of\n",
    "                  the window for each dimension of the input tensor.\n",
    "        strides : A list of ints that has length >= 4. The stride\n",
    "                  of the sliding window for each dimension of the\n",
    "                  input tensor.\n",
    "    \n",
    "    Returns:\n",
    "        A pooled image.\n",
    "    \"\"\"\n",
    "\n",
    "    pooled_image = tf.nn.max_pool(x, ksize=ksize, strides=strides, padding='SAME')\n",
    "\n",
    "    return pooled_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 畳込みとプーリング処理の途中経過を保持するクラスを実装する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FILTER_CONF = {\n",
    "    'height': 3,\n",
    "    'width': 3,\n",
    "    'channels': 1,\n",
    "    'num': 1\n",
    "}\n",
    "\n",
    "class ConvNetProgressHolder(object):\n",
    "\n",
    "    \"\"\"Holder of convoluted images and pooled image.\n",
    "    \n",
    "    This class is used like the struct of C language.\n",
    "    This has no methods.\n",
    "    \n",
    "    Attributes:\n",
    "        input_data (Tensor) : An image that is applied to convolution and pooling.\n",
    "        conv (list)         : The list of convoluted images, each images are Tensor objects.\n",
    "        pool (Tensor)       : A image that is pooled after convolutional layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.input_data = None\n",
    "        self.conv = []\n",
    "        self.pool = None\n",
    "\n",
    "    \n",
    "def apply_vgg_network_unit(x, channels, num_conv):\n",
    "\n",
    "    \"\"\"Apply VGG Network From a Convolutional Layer to Max Pooling Layer.\n",
    "\n",
    "    Table 1 of Simonyan and Zisserman(2015) is separated by 5 parts,\n",
    "    each parts is from an input data or a pooled data at previous part\n",
    "    to a maxpool.\n",
    "    This function provides to apply a that part.\n",
    "    This will apply recursively.\n",
    "    \n",
    "    Args:\n",
    "        x (Tensor)     : An input data or A Max pooled data returned by this function.\n",
    "        channels (int) : A number of channels described at Table 1 of\n",
    "                         Simonyan and Zisserman(2015).\n",
    "        num_conv (int) : A number of applying covolutional layers.\n",
    "                         See Simonyan and Zisserman(2015) for detail.\n",
    "\n",
    "    Returns:\n",
    "        A ConvNetProgressHolder object.\n",
    "    \"\"\"\n",
    "\n",
    "    if num_conv < 2:\n",
    "        raise ValueError('num_conv must be >= 2.')\n",
    "\n",
    "    conv_holder = ConvNetProgressHolder()\n",
    "    conv_holder.input_data = x\n",
    "\n",
    "    conv = calculate_convolutional_layer(\n",
    "        x=conv_holder.input_data,\n",
    "        filter_conf=FILTER_CONF,\n",
    "        output_channels=channels\n",
    "    )\n",
    "    conv_holder.conv.append(conv)\n",
    "\n",
    "    for i in range(1, num_conv):\n",
    "        conv = calculate_convolutional_layer(\n",
    "            x=conv_holder.conv[i - 1],\n",
    "            filter_conf=FILTER_CONF,\n",
    "            output_channels=channels\n",
    "        )\n",
    "        conv_holder.conv.append(conv)\n",
    "\n",
    "    conv_holder.pool = calculate_max_pooling_layer(\n",
    "        x=conv_holder.conv[i - 1],\n",
    "        ksize=[1, 1, 1, 1],\n",
    "        strides=[1, 1, 1, 1]\n",
    "    )\n",
    "\n",
    "    return conv_holder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGGの畳込みとプーリング層を構築する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unit1 = apply_vgg_network_unit(x=x, channels=64, num_conv=2)\n",
    "unit2 = apply_vgg_network_unit(x=unit1.pool, channels=128, num_conv=2)\n",
    "unit3 = apply_vgg_network_unit(x=unit2.pool, channels=256, num_conv=4)\n",
    "unit4 = apply_vgg_network_unit(x=unit3.pool, channels=512, num_conv=4)\n",
    "unit5 = apply_vgg_network_unit(x=unit4.pool, channels=512, num_conv=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
