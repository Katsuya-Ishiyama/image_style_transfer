{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Style Transferの実装\n",
    "\n",
    "--------------------------------\n",
    "Gatys, Ecker, and Bethge(2016)の画像スタイル変換を実装する。\n",
    "\n",
    "【参考文献】  \n",
    "L. A. Gatys, A. S. Ecker, and M. Bethge,  \n",
    "Image style transfer using convolutional neural networks,  \n",
    "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages, 2414-2423, 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CONTENT_FILE = '/home/ishiyama/image_style_transfer/image/input/test_input_01.JPG'\n",
    "STYLE_FILE = '/home/ishiyama/image_style_transfer/image/style/test_style_01.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Image(np.ndarray):\n",
    "\n",
    "    \"\"\"画像を扱うためのnumpy.ndarray\n",
    "    \n",
    "    XXX: 実装が大変なので一旦保留\n",
    "         画像の形状の情報を属性で取り出せるようにしたい\n",
    "    \"\"\"\n",
    "    \n",
    "    DATA_FORMAT_CHAR = {\n",
    "        'BATCH': 'N',\n",
    "        'HEIGHT': 'H',\n",
    "        'WIDTH': 'W',\n",
    "        'CHANNEL': 'C'\n",
    "    }\n",
    "\n",
    "    def __new__(subtype,\n",
    "                shape,\n",
    "                dtype=float,\n",
    "                buffer=None,\n",
    "                offset=0,\n",
    "                strides=None,\n",
    "                order=None,\n",
    "                data_format='NHWC'):\n",
    "\n",
    "        super(__class__, self).__new__(subtype, shape, dtype, buffer, offset, strides, order)\n",
    "\n",
    "        self.data_format = data_format\n",
    "        num_batch, num_height, num_width, num_channel = self._get_image_shape(data_format=data_format)\n",
    "        self.num_batch = num_batch\n",
    "        self.num_height = num_height\n",
    "        self.num_width = num_width\n",
    "        self.num_channel = num_channel\n",
    "\n",
    "    def _get_image_shape(self, data_format):\n",
    "        _image_shape = self.shape\n",
    "        idx_batch = self._get_index_data_format(data_format=data_format, data_type='BATCH'),\n",
    "        idx_height = self._get_index_data_format(data_format=data_format, data_type='HEIGHT')\n",
    "        idx_width = self._get_index_data_format(data_format=data_format, data_type='WIDTH')\n",
    "        idx_channel = self._get_index_data_format(data_format=data_format, data_type='CHANNEL')\n",
    "        reordered_image_shape = (_image_shape[idx_batch],\n",
    "                                 _image_shape[idx_height],\n",
    "                                 _image_shape[idx_width],\n",
    "                                 _image_shape[idx_channel])\n",
    "        return reordered_image_shape\n",
    "\n",
    "    def _get_index_data_format(self, data_format, data_type):\n",
    "        idx = data_format.find(__class__.DATA_FORMAT_CHAR[data_type])\n",
    "        if idx == -1:\n",
    "            raise ValueError('data type \"{}\" is not available.'.format(data_type))\n",
    "\n",
    "        return idx\n",
    "\n",
    "    @classmethod\n",
    "    def reshape(self, *args):\n",
    "        self = __class__(super(__class__, self).reshape(args))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_images_as_jpeg(file):\n",
    "    \n",
    "    \"\"\"\n",
    "    JPEG Image Reader\n",
    "    \n",
    "    This function reads the content and style images as JPEG format.\n",
    "    These image data must be square for now, different height and\n",
    "    width will be able to supplied for future.\n",
    "    \n",
    "    Args:\n",
    "        file : str. A path of the image file.\n",
    "    \n",
    "    Returns:\n",
    "        A tuple. Each Elements are Tensor object of the read images.\n",
    "    \"\"\"\n",
    "    \n",
    "    filename_queue = tf.train.string_input_producer([file])\n",
    "    reader = tf.WholeFileReader()\n",
    "    key, value = reader.read(filename_queue)\n",
    "    image = tf.image.decode_jpeg(value)\n",
    "\n",
    "    # Read image is a Tensor object which tf.nn.conv2d cannot handle,\n",
    "    # so convert it to numpy.ndarray.\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    tf.train.start_queue_runners(sess)\n",
    "    # Returned array's shape will be (height, width, channel).\n",
    "    image_array_hwc = sess.run(image)\n",
    "    new_shape = [1]\n",
    "    new_shape.extend(image_array_hwc.shape)\n",
    "\n",
    "    # return image_array_chw\n",
    "    return image_array_hwc.reshape(new_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "content_image = read_images_as_jpeg(file=CONTENT_FILE)\n",
    "style_image = read_images_as_jpeg(file=STYLE_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGGを実装する\n",
    "------------------------\n",
    "画像の特徴量を抽出するアルゴリズムにはSimonyan and Zisserman(2015)で提案されたCNN(VGG19)の畳込み層とプーリング層が使われている。  \n",
    "ここでは、「TensorFlowで学ぶディープラーニング入門」の多層CNNの実装を参考にVGG19を構築する。  \n",
    "\n",
    "【参考文献】  \n",
    "K. Simonyan and A. Zisserman, Very Deep Convolutional Networks For Large-Scale Image Recognition, arXiv: 1409.1556v6, 2015  \n",
    "中井悦司, TensorFlowで学ぶディープラーニング入門〜畳み込みニューラルネットワーク徹底解説, マイナビ出版, 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 畳み込み層を実装する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "\n",
    "def calculate_convolutional_layer(x, filter_height, filter_width, output_channels):\n",
    "\n",
    "    \"\"\"\n",
    "    Executeing a convolutional layer task.\n",
    "    \n",
    "    Args:\n",
    "        x                     : An image data.\n",
    "        filter_height   (int) : A height of each filters.\n",
    "        filter_width    (int) : A width of each filters.\n",
    "        output_channels (int) : A number of channels which must be outputed.\n",
    "\n",
    "    Returns:\n",
    "        An activation of an convolutional layer.\n",
    "    \"\"\"\n",
    "\n",
    "    if ((not isinstance(filter_height, int))\n",
    "        or (not isinstance(filter_width, int))\n",
    "        or (not isinstance(output_channels, int))):\n",
    "        raise TypeError('\"filter_height\" and \"filter_width\" and \"output_channels\" '\n",
    "                        'must be integer.')\n",
    "\n",
    "    # TODO: 入力画像の縦、横、チャンネル数を属性で取得できるようにする\n",
    "    # 例） input_channels = x.num_channels\n",
    "    input_channels = int(x.shape[-1])\n",
    "    filter_value = 1 / float(filter_height * filter_width)\n",
    "    epsilon = filter_value / 10.0\n",
    "    W = tf.Variable(\n",
    "        tf.random_uniform(\n",
    "            shape=[filter_height,\n",
    "                   filter_width,\n",
    "                   input_channels,\n",
    "                   output_channels],\n",
    "            minval=filter_value - epsilon,\n",
    "            maxval=filter_value + epsilon\n",
    "        )\n",
    "    )\n",
    "    h = tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    b = tf.Variable(tf.constant(0.1, shape=[output_channels]))\n",
    "    convoluted_image = tf.nn.relu(h + b)\n",
    "\n",
    "    return convoluted_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [1, 477, 477, 3])\n",
    "test_model = calculate_convolutional_layer(\n",
    "    x=x,\n",
    "    filter_height=3,\n",
    "    filter_width=3,\n",
    "    output_channels=1\n",
    ")\n",
    "sess = tf.InteractiveSession()\n",
    "# tf.Session()だと、sess.runで返ってくる行列の要素がすべて0だった。\n",
    "# TODO: Sessionメソッド と InteractiveSessionメソッドの違いを調べる\n",
    "# sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "test_result = sess.run(test_model, feed_dict={x: content_image})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 477, 477, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 114.27301025],\n",
       "         [ 173.50660706],\n",
       "         [ 174.29614258],\n",
       "         ..., \n",
       "         [ 480.10452271],\n",
       "         [ 481.24404907],\n",
       "         [ 323.35064697]],\n",
       "\n",
       "        [[ 166.4135437 ],\n",
       "         [ 252.51049805],\n",
       "         [ 254.13793945],\n",
       "         ..., \n",
       "         [ 717.27807617],\n",
       "         [ 718.06811523],\n",
       "         [ 481.68475342]],\n",
       "\n",
       "        [[ 158.3868866 ],\n",
       "         [ 241.47108459],\n",
       "         [ 244.67269897],\n",
       "         ..., \n",
       "         [ 718.62445068],\n",
       "         [ 718.66595459],\n",
       "         [ 481.57723999]],\n",
       "\n",
       "        ..., \n",
       "        [[ 448.78652954],\n",
       "         [ 674.51971436],\n",
       "         [ 673.51464844],\n",
       "         ..., \n",
       "         [ 672.70135498],\n",
       "         [ 670.52099609],\n",
       "         [ 449.13308716]],\n",
       "\n",
       "        [[ 449.70996094],\n",
       "         [ 676.00695801],\n",
       "         [ 674.76342773],\n",
       "         ..., \n",
       "         [ 673.22253418],\n",
       "         [ 671.83905029],\n",
       "         [ 450.01858521]],\n",
       "\n",
       "        [[ 297.78442383],\n",
       "         [ 444.92471313],\n",
       "         [ 443.75894165],\n",
       "         ..., \n",
       "         [ 443.45913696],\n",
       "         [ 442.98733521],\n",
       "         [ 296.54943848]]]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Maxプーリング層を実装する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_max_pooling_layer(x, ksize, strides):\n",
    "\n",
    "    \"\"\"Wrapper function of tf.nn.max_pool.\n",
    "    \n",
    "    Args:\n",
    "        x       : A Tensor produced by calculate_convolutional_layer.\n",
    "        ksize   : A list of ints that has length >= 4. The size of\n",
    "                  the window for each dimension of the input tensor.\n",
    "        strides : A list of ints that has length >= 4. The stride\n",
    "                  of the sliding window for each dimension of the\n",
    "                  input tensor.\n",
    "    \n",
    "    Returns:\n",
    "        A pooled image.\n",
    "    \"\"\"\n",
    "\n",
    "    pooled_image = tf.nn.max_pool(x, ksize=ksize, strides=strides, padding='SAME')\n",
    "\n",
    "    return pooled_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 畳込みとプーリング処理の途中経過を保持するクラスを実装する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FeatureMapHolder(object):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.conv = []\n",
    "        self.pool = None\n",
    "\n",
    "    def set_conv(self, mat):\n",
    "\n",
    "        self.conv.append(mat)\n",
    "\n",
    "    def get_conv(self, idx):\n",
    "\n",
    "        if idx is None:\n",
    "            raise ValueError('idx is required.')\n",
    "\n",
    "        if not isinstance(idx, int):\n",
    "            raise ValueError('idx must be an integer.')\n",
    "\n",
    "        return self.conv[idx]\n",
    "\n",
    "    def set_pool(self, mat):\n",
    "\n",
    "        self.pool = mat\n",
    "\n",
    "    def get_pool(self):\n",
    "\n",
    "        return self.pool\n",
    "\n",
    "    def get(self, type, idx=None):\n",
    "\n",
    "        if type == 'pool':\n",
    "            return self.get_pool()\n",
    "        elif type == 'conv':\n",
    "            return self.get_conv(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FILTER_CONF = {\n",
    "    'conv': {\n",
    "        'height': 3,\n",
    "        'width': 3\n",
    "    },\n",
    "    'maxpool': {\n",
    "        'height': 2,\n",
    "        'width': 2\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "def apply_vgg_network_unit(x, channels, num_conv):\n",
    "\n",
    "    \"\"\"Apply VGG Network From a Convolutional Layer to Max Pooling Layer.\n",
    "\n",
    "    Table 1 of Simonyan and Zisserman(2015) is separated by 5 parts,\n",
    "    each parts is from an input data or a pooled data at previous part\n",
    "    to a maxpool.\n",
    "    This function provides to apply a that part.\n",
    "    This will apply recursively.\n",
    "    \n",
    "    Args:\n",
    "        x (Tensor)     : An input data or A Max pooled data returned by this function.\n",
    "        channels (int) : A number of channels described at Table 1 of\n",
    "                         Simonyan and Zisserman(2015).\n",
    "        num_conv (int) : A number of applying covolutional layers.\n",
    "                         See Simonyan and Zisserman(2015) for detail.\n",
    "\n",
    "    Returns:\n",
    "        A ConvNetProgressHolder object.\n",
    "    \"\"\"\n",
    "\n",
    "    if num_conv < 2:\n",
    "        raise ValueError('num_conv must be >= 2.')\n",
    "\n",
    "    feature_maps = FeatureMapHolder()\n",
    "\n",
    "    conv = calculate_convolutional_layer(\n",
    "        x=x,\n",
    "        filter_height=FILTER_CONF['conv']['height'],\n",
    "        filter_width=FILTER_CONF['conv']['width'],\n",
    "        output_channels=channels\n",
    "    )\n",
    "    feature_maps.set_conv(conv)\n",
    "\n",
    "    for i in range(1, num_conv):\n",
    "        conv = calculate_convolutional_layer(\n",
    "            x=feature_maps.get(type='conv', idx=i-1),\n",
    "            filter_height=FILTER_CONF['conv']['height'],\n",
    "            filter_width=FILTER_CONF['conv']['width'],\n",
    "            output_channels=channels\n",
    "        )\n",
    "        feature_maps.set_conv(conv)\n",
    "\n",
    "    pool = calculate_max_pooling_layer(\n",
    "        x=feature_maps.get('conv', idx=i-1),\n",
    "        ksize=[1, FILTER_CONF['maxpool']['height'], FILTER_CONF['maxpool']['width'], 1],\n",
    "        strides=[1, 2, 2, 1]\n",
    "    )\n",
    "    feature_maps.set_pool(pool)\n",
    "\n",
    "    return feature_maps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGGの畳込みとプーリング層を構築する\n",
    "\n",
    "VGGの論文に従い、複数回の畳込み処理と1回のMAXプーリング処理を1セットとして、それを5回繰り返す。  \n",
    "今回実装する画像変換アルゴリズムでは、この処理の途中経過を使うため、使いたい部分をリストで１つにまとめてsess.run()に投げる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 例\n",
    "x = tf.placeholder(tf.float32, [1, 477, 477, 3])\n",
    "unit1 = apply_vgg_network_unit(x=x, channels=2, num_conv=2)\n",
    "unit2 = apply_vgg_network_unit(x=unit1.get(type='pool'), channels=4, num_conv=2)\n",
    "unit3 = apply_vgg_network_unit(x=unit2.get(type='pool'), channels=8, num_conv=4)\n",
    "unit4 = apply_vgg_network_unit(x=unit3.get(type='pool'), channels=16, num_conv=4)\n",
    "unit5 = apply_vgg_network_unit(x=unit4.get(type='pool'), channels=32, num_conv=4)\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 使いたい過程をリストでまとめてsess.runに投げると特徴量抽出の\n",
    "# 途中経過を取り出せる\n",
    "result_unit2_conv, result_unit5_conv, result_unit5_pool = sess.run(\n",
    "    [unit2.get(type='conv', idx=1), unit5.get(type='conv', idx=2), unit5.get(type='pool')],\n",
    "    feed_dict={x: content_image}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NETWORK_CONF = {\n",
    "    'layer1': {\n",
    "        'num_channels': 2,\n",
    "        'num_conv': 2\n",
    "    },\n",
    "    'layer2': {\n",
    "        'num_channels': 4,\n",
    "        'num_conv': 2\n",
    "    },\n",
    "    'layer3': {\n",
    "        'num_channels': 8,\n",
    "        'num_conv': 4\n",
    "    },\n",
    "    'layer4': {\n",
    "        'num_channels': 16,\n",
    "        'num_conv': 4\n",
    "    },\n",
    "    'layer5': {\n",
    "        'num_channels': 32,\n",
    "        'num_conv': 4\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_feature_map(image, extract=None, run=True):\n",
    "\n",
    "    x = tf.placeholder(tf.float32, image.shape)\n",
    "\n",
    "    layers = []\n",
    "\n",
    "    layer1 = apply_vgg_network_unit(\n",
    "        x=x,\n",
    "        channels=NETWORK_CONF['layer1']['num_channels'],\n",
    "        num_conv=NETWORK_CONF['layer1']['num_conv']\n",
    "    )\n",
    "    layers.append(layer1)\n",
    "\n",
    "    layer2 = apply_vgg_network_unit(\n",
    "        x=layer1.get(type='pool'),\n",
    "        channels=NETWORK_CONF['layer2']['num_channels'],\n",
    "        num_conv=NETWORK_CONF['layer2']['num_conv']\n",
    "    )\n",
    "    layers.append(layer2)\n",
    "    \n",
    "    layer3 = apply_vgg_network_unit(\n",
    "        x=layer2.get(type='pool'),\n",
    "        channels=NETWORK_CONF['layer3']['num_channels'],\n",
    "        num_conv=NETWORK_CONF['layer3']['num_conv']\n",
    "    )\n",
    "    layers.append(layer3)\n",
    "    \n",
    "    layer4 = apply_vgg_network_unit(\n",
    "        x=layer3.get(type='pool'),\n",
    "        channels=NETWORK_CONF['layer4']['num_channels'],\n",
    "        num_conv=NETWORK_CONF['layer4']['num_conv']\n",
    "    )\n",
    "    layers.append(layer4)\n",
    "    \n",
    "    layer5 = apply_vgg_network_unit(\n",
    "        x=layer4.get(type='pool'),\n",
    "        channels=NETWORK_CONF['layer5']['num_channels'],\n",
    "        num_conv=NETWORK_CONF['layer5']['num_conv']\n",
    "    )\n",
    "    layers.append(layer5)\n",
    "\n",
    "    # モデルの構築のみの場合(run=True)はlayersを返却して終了\n",
    "    if not run:\n",
    "        return layers\n",
    "\n",
    "    if extract is None:\n",
    "        raise ValueError('extract is required if run is True.')\n",
    "\n",
    "    sess = tf.InteractiveSession()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    extract_layers = []\n",
    "    for ext in extract:\n",
    "        if len(ext) == 2:\n",
    "            type, idx_layer = ext\n",
    "            extract_layers.append(layers[idx_layer].get(type))\n",
    "        elif len(ext) == 3:\n",
    "            type, idx_layer, idx_conv = ext\n",
    "            extract_layers.append(layers[idx_layer].get(type, idx_conv))\n",
    "        else:\n",
    "            raise ValueError('Format of extract is not available: {}'.format(ext))\n",
    "\n",
    "    result_list = sess.run(extract_layers, feed_dict={x: image})\n",
    "\n",
    "    return result_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "conv1_1, conv2_2, pool4 = extract_feature_map(\n",
    "    image=content_image,\n",
    "    extract=[('conv', 0, 0), ('conv', 1, 1), ('pool', 3)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 画像を合成する処理を作る\n",
    "VGGによってスタイルを変換する画像とスタイルのリファレンスとなる画像の特徴量を抽出できるようになった。  \n",
    "今度はこの特徴量を用いて画像を実際に合成する部分を作っていく。  \n",
    "\n",
    "このアルゴリズムは変換したいスタイルに寄せるための損失関数$L_{style}$と  \n",
    "変換する画像の内容に寄せるための損失関数$L_{content}$をそれぞれ計算して、  \n",
    "\\begin{equation} L_{total} = \\alpha L_{content} + \\beta L_{style} \\end{equation}\n",
    "を最適化することで画像を合成する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------\n",
    "# 変換したいスタイルに寄せるための損失関数$L_{style}$を実装する\n",
    "変換したいスタイルに寄せるための損失関数$L_{style}$は次で計算する：\n",
    "\\begin{align}\n",
    "    L_{style} &= \\sum_{l=1}^{L} w_{l} E_{l} \\\\\n",
    "        E_{l} &= \\frac{1}{4N_{l}^{2}M_{l}^{2}} \\sum_{i=1}^{N_{l}} \\sum_{j=1}^{N_{l}} (G_{ij}^{l} - A_{ij}^{l})^{2}\n",
    "\\end{align}\n",
    "ただし、$N_{l}$は層$l$の畳込みフィルターの数、\n",
    "$M_{l}$は層$l$の畳込みフィルターのサイズで$M_{l} = \\text{フィルターの高さ} \\times \\text{フィルターの幅}$、\n",
    "$A_{ij}^{l}$はスタイル画像から抽出した特徴量で計算したグラム行列$A^{l}$の$(i, j)$成分、\n",
    "同様に、$G_{ij}^{l}$は合成後の画像から抽出した特徴量で計算したグラム行列$G^{l}$の$(i, j)$成分である。\n",
    "$G_{ij}^{l}$は次で計算する。  \n",
    "$G^{l}$は$(N_{l}, N_{l})$型行列で\n",
    "\\begin{equation}\n",
    "    G_{ij}^{l} = \\sum_{k=1}^{M_{l}} F_{ik}^{l}F_{jk}^{l}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_nhwc_to_nchw(image):\n",
    "    return image.transpose([0, 3, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 32, 30, 30)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converted = convert_nhwc_to_nchw(result_unit5_conv)\n",
    "converted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flatten(image):\n",
    "    num_batch, num_channel, num_height, num_width = image.shape\n",
    "#    if num_batch != 1:\n",
    "#        raise ValueError('Not assumed batch size has been ocurred.')\n",
    "    return image.reshape([num_batch, num_channel, num_height * num_width])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### flattenのテスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 0,  1],\n",
       "         [ 2,  3],\n",
       "         [ 4,  5],\n",
       "         [ 6,  7]],\n",
       "\n",
       "        [[ 8,  9],\n",
       "         [10, 11],\n",
       "         [12, 13],\n",
       "         [14, 15]],\n",
       "\n",
       "        [[16, 17],\n",
       "         [18, 19],\n",
       "         [20, 21],\n",
       "         [22, 23]]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data1 = np.arange(1 * 3 * 4 * 2).reshape([1, 3, 4, 2])\n",
    "test_data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0,  1,  2,  3,  4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11, 12, 13, 14, 15],\n",
       "        [16, 17, 18, 19, 20, 21, 22, 23]]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatten(test_data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 0,  1],\n",
       "         [ 2,  3],\n",
       "         [ 4,  5],\n",
       "         [ 6,  7]],\n",
       "\n",
       "        [[ 8,  9],\n",
       "         [10, 11],\n",
       "         [12, 13],\n",
       "         [14, 15]],\n",
       "\n",
       "        [[16, 17],\n",
       "         [18, 19],\n",
       "         [20, 21],\n",
       "         [22, 23]]],\n",
       "\n",
       "\n",
       "       [[[24, 25],\n",
       "         [26, 27],\n",
       "         [28, 29],\n",
       "         [30, 31]],\n",
       "\n",
       "        [[32, 33],\n",
       "         [34, 35],\n",
       "         [36, 37],\n",
       "         [38, 39]],\n",
       "\n",
       "        [[40, 41],\n",
       "         [42, 43],\n",
       "         [44, 45],\n",
       "         [46, 47]]]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data2 = np.arange(2 * 3 * 4 * 2).reshape([2, 3, 4, 2])\n",
    "test_data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0,  1,  2,  3,  4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11, 12, 13, 14, 15],\n",
       "        [16, 17, 18, 19, 20, 21, 22, 23]],\n",
       "\n",
       "       [[24, 25, 26, 27, 28, 29, 30, 31],\n",
       "        [32, 33, 34, 35, 36, 37, 38, 39],\n",
       "        [40, 41, 42, 43, 44, 45, 46, 47]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatten(test_data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# グラム行列を計算する\n",
    "def calculate_gram_matrix(x):\n",
    "    return np.dot(x, x.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  1.72774497e+12,   1.71310606e+12,   1.71771534e+12, ...,\n",
       "           2.04983711e+12,   2.06964392e+12,   2.06642859e+12],\n",
       "        [  3.19720443e+12,   3.17136621e+12,   3.17231753e+12, ...,\n",
       "           3.36004750e+12,   3.38134932e+12,   3.37660713e+12],\n",
       "        [  4.26700073e+12,   4.23316842e+12,   4.23089118e+12, ...,\n",
       "           3.69687554e+12,   3.71591007e+12,   3.71104154e+12],\n",
       "        ..., \n",
       "        [  4.77729286e+12,   4.74064146e+12,   4.73164310e+12, ...,\n",
       "           4.39728315e+12,   4.41955333e+12,   4.41371905e+12],\n",
       "        [  3.92295009e+12,   3.89338995e+12,   3.88214948e+12, ...,\n",
       "           3.65187223e+12,   3.66564449e+12,   3.66082182e+12],\n",
       "        [  2.26813621e+12,   2.25203690e+12,   2.23939710e+12, ...,\n",
       "           2.12420749e+12,   2.12506536e+12,   2.12239201e+12]]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattened = flatten(result_unit5_conv)\n",
    "flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gram_matrix = calculate_gram_matrix(flattened[0])\n",
    "# gram_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 30)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gram_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "content_conv4_2 = extract_feature_map(image=content_image, extract=[('conv', 3, 1)])\n",
    "style_conv1_1, style_conv2_1, style_conv3_1, style_conv4_1, style_conv5_1 = extract_feature_map(\n",
    "    image=style_image,\n",
    "    extract=[\n",
    "        ('conv', 0, 0),\n",
    "        ('conv', 1, 0),\n",
    "        ('conv', 2, 0),\n",
    "        ('conv', 3, 0),\n",
    "        ('conv', 4, 0)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 損失関数を構成する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "creating_image = tf.placeholder(tf.float32, [1, 477, 477, 3])\n",
    "feature_map = extract_feature_map(\n",
    "    image=creating_image,\n",
    "    run=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Relu_97:0' shape=(1, 477, 477, 2) dtype=float32>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_map[0].get(type='conv', idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
